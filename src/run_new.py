from config import PipelineConfig
from pipeline import RouterEvaluationPipeline, get_task_score
from train_router import (
    generate_logits, set_random_seed,
    complete_probe_training_pipeline_with_mixed_datasets,
    generate_query_embeddings,
    train_embedding_mlp_model,
    train_deberta_router,
    prepare_deberta_training_file
)
import os
config_env = PipelineConfig.from_yaml()
if config_env.inference.cuda_visible_devices:
    os.environ["CUDA_VISIBLE_DEVICES"] = config_env.inference.cuda_visible_devices
import copy
import json
import argparse
import glob
import re
from pathlib import Path
from datetime import datetime
import torch


def save_training_history(history, probe_type, task_list, max_samples=None, save_dir="probe_save/loss"):
    """
    Save training loss and accuracy history to a JSON file

    Args:
        history: Dictionary containing train_losses, val_losses, and best_val_loss
        probe_type: Type of probe being trained
        task_list: List of tasks used for training
        max_samples: Maximum number of samples used for training
        save_dir: Directory to save the history file
    """
    save_path = Path(save_dir)
    save_path.mkdir(parents=True, exist_ok=True)

    # Create filename with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    tasks_str = "_".join(task_list)
    filename = f"{tasks_str}_{probe_type}_{timestamp}.json"
    filepath = save_path / filename

    # Extract training results from history dict
    # The history dict has a 'training_results' key that contains the actual loss data
    training_results = history.get('training_results', {})
    train_losses = training_results.get('train_losses', [])
    val_losses = training_results.get('val_losses', [])
    val_accuracies = training_results.get('val_accuracies', [])
    val_aurocs = training_results.get('val_aurocs', [])
    learning_rates = training_results.get('learning_rates', [])
    best_val_loss = training_results.get('best_val_loss', float('inf'))
    best_val_acc = training_results.get('best_val_acc', 0.0)
    best_val_auroc = training_results.get('best_val_auroc', 0.0)
    initial_lr = training_results.get('initial_lr', 0.0)

    # Save to JSON
    save_data = {
        "probe_type": probe_type,
        "tasks": task_list,
        "datasets": task_list,  # æ·»åŠ æ•°æ®é›†ä¿¡æ¯(ä¸tasksç›¸åŒ)
        "max_samples": max_samples,  # æ·»åŠ ä½¿ç”¨çš„æœ€å¤§æ ·æœ¬æ•°
        "timestamp": timestamp,
        "initial_lr": initial_lr,
        "best_val_loss": best_val_loss,
        "best_val_acc": best_val_acc,
        "best_val_auroc": best_val_auroc,
        "epochs": len(train_losses),
        "train_losses": train_losses,
        "val_losses": val_losses,
        "val_accuracies": val_accuracies,
        "val_aurocs": val_aurocs,
        "learning_rates": learning_rates,
    }

    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, indent=2, ensure_ascii=False)

    print(f"ğŸ“Š Training history saved to: {filepath}")
    print(f"   Datasets used: {', '.join(task_list)}")
    if max_samples:
        print(f"   Max samples: {max_samples} ({max_samples/1000:.1f}k)")
    return str(filepath)


def batch_evaluate_probes(base_config, probe_configs, eval_tasks):
    # ä»é…ç½®ä¸­è·å–æ¨¡å‹åç§°
    model_name = _extract_model_name_from_path(base_config.inference.weak_model_path)

    for i, probe_config in enumerate(probe_configs):
        config_copy = copy.deepcopy(base_config)
        config_copy.router.checkpoint_path = probe_config['checkpoint_path']
        config_copy.router.probe_type = probe_config['probe_type']
        config_copy.metric_results_dir = probe_config['metric_results_dir']
        print(f"\n{'='*60}")
        print(f"Running evaluation {i+1}/{len(probe_configs)}")
        print(f"Probe type: {probe_config['probe_type']}")
        print(f"Checkpoint: {probe_config['checkpoint_path']}")
        print(f"{'='*60}")
        pipeline = RouterEvaluationPipeline(config_copy)

        for task in eval_tasks:
            print(f"\nEvaluating task: {task}")

            hidden_states_file = _build_hs_path(task, model_name)

            if not os.path.exists(hidden_states_file):
                print(f"Warning: Hidden states file not found: {hidden_states_file}")
                continue

            datasets = [f"{task}"]

            pipeline.evaluate_complete_pipeline(
                hidden_states_file=hidden_states_file,
                datasets=datasets
            )


def _extract_model_name_from_path(model_path: str) -> str:
    """ä»æ¨¡å‹è·¯å¾„ä¸­æå–æ¨¡å‹åç§°"""
    return os.path.basename(model_path.rstrip('/'))


def _build_hs_path(task: str, model_name: str = None):
    """æ„å»ºhidden statesæ–‡ä»¶è·¯å¾„"""
    if model_name is None:
        # ä»é…ç½®æ–‡ä»¶ä¸­è¯»å– weak_model_path
        config = PipelineConfig.from_yaml()
        model_name = _extract_model_name_from_path(config.inference.weak_model_path)

    base_dir = os.path.join("..", "hs")
    if task.startswith("mmlu_pro_"):
        base_dir = os.path.join(base_dir, "mmlu_pro")
    return os.path.join(base_dir, f"{model_name}_{task}.pt")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='CoBench Router Evaluation and Training Framework',
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    parser.add_argument("--mode", type=str, required=True,
                       choices=["get_scores", "get_logits", "get_query_emb",
                                "train", "train_embedding_mlp",
                                "eval_probe", "eval_embedding_mlp", "eval_deberta",
                                "eval_max_k", 
                                "self_based", "logits_based_routers"],
                       help="è¿è¡Œæ¨¡å¼")

    parser.add_argument("--datasets", type=str, nargs="+", default=None,
                       help="è¦å¤„ç†çš„æ•°æ®é›†åˆ—è¡¨ (ç©ºæ ¼åˆ†éš”)")

    parser.add_argument("--probe_types", type=str, nargs="+",
                       default=["hs_last_mlp", "mean", "max", "coe_dual_mlp"],
                       help="è®­ç»ƒæ¨¡å¼ä¸‹çš„ probe ç±»å‹")

    parser.add_argument("--max_samples", type=int, default=4000,
                       help="è®­ç»ƒæ—¶çš„æœ€å¤§æ ·æœ¬æ•°")

    parser.add_argument("--seed", type=int, default=42,
                       help="éšæœºç§å­")

    parser.add_argument("--save_loss_history", action="store_true",
                       help="åœ¨è®­ç»ƒæ—¶ä¿å­˜ loss å’Œ accuracy å†å²è®°å½•")

    parser.add_argument("--probe_dir", type=str, default=None,
                       help="Probe æ¨¡å‹ç›®å½•è·¯å¾„ï¼ˆç”¨äº eval_probe æ¨¡å¼ï¼‰")

    parser.add_argument("--query_embeddings_file", type=str, default=None,
                       help="EmbeddingMLP è®­ç»ƒ/è¯„ä¼°æ‰€éœ€çš„ query embedding pt æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--text_field", type=str, default="instruction",
                       help="ç”Ÿæˆ query embedding æ—¶ä½¿ç”¨çš„æ–‡æœ¬å­—æ®µ")
    parser.add_argument("--embed_batch_size", type=int, default=64,
                       help="ç”Ÿæˆ query embedding çš„ batch size")
    parser.add_argument("--embedding_checkpoint", type=str, default=None,
                       help="EmbeddingMLP checkpoint è·¯å¾„ï¼ˆè¯„ä¼°æˆ–è‡ªå®šä¹‰ä¿å­˜åï¼‰")

    args = parser.parse_args()
    mode = args.mode
    set_random_seed(args.seed)
    config = PipelineConfig().from_yaml()
    pipeline = RouterEvaluationPipeline(config)

    # ==================== æ¨¡å¼: get_scores ====================
    if mode == "get_scores":
        datasets = args.datasets 
        print(f"ğŸ¯ è·å–ä»¥ä¸‹æ•°æ®é›†çš„ scores: {datasets}")
        for task in datasets:
            print(f"\n{'='*60}")
            print(f"ğŸ“Š å¤„ç†ä»»åŠ¡: {task}")
            print(f"{'='*60}")
            try:
                score = get_task_score(config, task=task)
                print(f"âœ… {task} å®Œæˆ")
                if score:
                    print(f"   Score path: {score}")
            except Exception as e:
                print(f"âŒ {task} å¤±è´¥: {e}")

    # ==================== æ¨¡å¼: get_logits ====================
    elif mode == "get_logits":
        datasets = args.datasets 
        print(f"ğŸ¯ ç”Ÿæˆä»¥ä¸‹æ•°æ®é›†çš„ logits: {datasets}")

        for task in datasets:
            print(f"\n{'='*60}")
            print(f"ğŸ“Š å¤„ç†ä»»åŠ¡: {task}")
            print(f"{'='*60}")

            # æ„å»º task_path
            if task.startswith("mmlu_pro_"):
                task_path = os.path.join("./results/mmlu_pro", f"{task}.jsonl")
            else:
                task_path = os.path.join("./results", f"{task}.jsonl")

            if not os.path.exists(task_path):
                print(f"âš ï¸  è­¦å‘Š: ç»“æœæ–‡ä»¶ {task_path} ä¸å­˜åœ¨")
                continue

            try:
                generate_logits(config, task=task, task_path=task_path)
                print(f"âœ… {task} logits ç”Ÿæˆå®Œæˆ")
            except Exception as e:
                print(f"âŒ {task} logits ç”Ÿæˆå¤±è´¥: {e}")

    # ==================== æ¨¡å¼: get_query_emb ====================
    elif mode == "get_query_emb":
        datasets = args.datasets
        print(f"ğŸ¯ ç”Ÿæˆä»¥ä¸‹æ•°æ®é›†çš„ query embedding: {datasets}")

        for task in datasets:
            print(f"\n{'='*60}")
            print(f"ğŸ“Š å¤„ç†ä»»åŠ¡: {task}")
            print(f"{'='*60}")

            if task.startswith("mmlu_pro_"):
                task_path = os.path.join("./results/mmlu_pro", f"{task}.jsonl")
            else:
                task_path = os.path.join("./results", f"{task}.jsonl")

            if not os.path.exists(task_path):
                print(f"âš ï¸  è­¦å‘Š: ç»“æœæ–‡ä»¶ {task_path} ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ get_scores")
                continue

            try:
                save_path = generate_query_embeddings(
                    task_path,
                    output_dir=config.training.query_embedding_output_dir,
                    batch_size=args.embed_batch_size,
                    text_field=args.text_field
                )
                print(f"âœ… {task} query embedding å·²ä¿å­˜åˆ° {save_path}")
            except Exception as e:
                print(f"âŒ {task} ç”Ÿæˆ query embedding å¤±è´¥: {e}")

    # ==================== æ¨¡å¼: train ====================
    elif mode == "train":
        max_samples = args.max_samples
        save_history = args.save_loss_history

        if config.router.router_type in ["trained_deberta", "deberta"]:
            train_path = config.training.deberta_train_path
            val_path = config.training.deberta_val_path

            if args.datasets:
                try:
                    train_path = prepare_deberta_training_file(config, args.datasets)
                except Exception as e:
                    print(f"âŒ DeBERTa è®­ç»ƒæ•°æ®å‡†å¤‡å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    exit(1)

            print("ğŸš€ å¼€å§‹è®­ç»ƒ DeBERTa Router")
            print(f"ğŸ“Š è®­ç»ƒæ•°æ®: {train_path}")
            if val_path:
                print(f"ğŸ“Š éªŒè¯æ•°æ®: {val_path}")

            try:
                output_dir = train_deberta_router(config, train_path=train_path, val_path=val_path)
                final_ckpt = Path(output_dir) / f"checkpoint_epoch_{config.training.deberta_epochs}"
                print(f"âœ… DeBERTa è®­ç»ƒå®Œæˆï¼Œè¾“å‡ºç›®å½•: {output_dir}")
                print(f"   æœŸæœ›æœ€ç»ˆ checkpoint: {final_ckpt}")
            except Exception as e:
                print(f"âŒ DeBERTa è®­ç»ƒå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

        elif config.router.router_type == "embedding_mlp":
            # ä½¿ç”¨é…ç½®ä¸­çš„ embedding_files è®­ç»ƒ EmbeddingMLP
            files = config.router.embedding_files or []
            if not files:
                print("âŒ é…ç½® router.embedding_files ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒ EmbeddingMLP")
                exit(1)
            all_data = []
            for fp in files:
                if not os.path.exists(fp):
                    print(f"âš ï¸  åµŒå…¥æ–‡ä»¶ä¸å­˜åœ¨: {fp}")
                    continue
                data = torch.load(fp, map_location="cpu", weights_only=False)
                if isinstance(data, dict) and "data" in data:
                    data = data["data"]
                print(f"ğŸ“‚ {fp} -> {len(data)} æ¡æ ·æœ¬")
                all_data.extend(data)
            print(f"ğŸ”¢ å·²åŠ è½½ embedding æ ·æœ¬æ€»æ•°: {len(all_data)}ï¼Œæ¥è‡ª {len(files)} ä¸ªæ–‡ä»¶")
            if len(all_data) == 0:
                print("âŒ æœªåŠ è½½åˆ°ä»»ä½•æ ·æœ¬")
                exit(1)
            if max_samples and len(all_data) > max_samples:
                all_data = random.sample(all_data, max_samples)

            split = int(len(all_data) * 0.8)
            train_data, val_data = all_data[:split], all_data[split:]

            sample = train_data[0]
            sample_emb = sample.get("query_embedding", None)
            if sample_emb is None:
                sample_emb = sample.get("embedding", None)
            if isinstance(sample_emb, torch.Tensor):
                input_dim = sample_emb.shape[-1]
            elif hasattr(sample_emb, "__len__"):
                input_dim = len(sample_emb)
            else:
                raise ValueError("æ— æ³•è§£ææ ·æœ¬ embedding ç»´åº¦")

            hidden_dims = config.training.embedding_hidden_dims
            dropout = config.training.embedding_dropout
            epochs = config.training.epochs
            batch_size = config.training.batch_size
            lr = config.training.learning_rate

            # è‡ªå®šä¹‰ä¿å­˜è·¯å¾„ï¼šä¼˜å…ˆ router.checkpoint_pathï¼Œå¦åˆ™è½åˆ° training.embedding_mlp_save_path
            if config.router.checkpoint_path:
                save_path = Path(config.router.checkpoint_path)
                save_path.parent.mkdir(parents=True, exist_ok=True)
            else:
                save_dir = Path(config.training.embedding_mlp_save_path)
                save_dir.mkdir(parents=True, exist_ok=True)
                save_name = f"embedding_mlp_{Path(files[0]).stem}.pt"
                save_path = save_dir / save_name

            print(f"ğŸš€ è®­ç»ƒ EmbeddingMLP")
            print(f"   æ ·æœ¬æ•°: {len(all_data)} (train {len(train_data)}, val {len(val_data)})")
            print(f"   hidden_dims={hidden_dims}, dropout={dropout}, epochs={epochs}, batch_size={batch_size}, lr={lr}")
            print(f"   ä¿å­˜åˆ°: {save_path}")

            res = train_embedding_mlp_model(
                train_data=train_data,
                val_data=val_data,
                input_dim=input_dim,
                save_path=str(save_path),
                hidden_dims=hidden_dims,
                dropout=dropout,
                epochs=epochs,
                batch_size=batch_size,
                lr=lr
            )
            print(f"âœ… EmbeddingMLP è®­ç»ƒå®Œæˆï¼Œæœ€ä¼˜ val_loss={res['best_val_loss']:.4f}")

        else:
            # åŸ probe è®­ç»ƒæµç¨‹
            datasets = args.datasets 
            probe_types = args.probe_types

            print(f"ğŸš€ å¼€å§‹è®­ç»ƒ Probe æ¨¡å‹")
            print(f"ğŸ“Š æ•°æ®é›†: {datasets}")
            print(f"ğŸ”§ Probe ç±»å‹: {probe_types}")
            print(f"ğŸ“ˆ æœ€å¤§æ ·æœ¬æ•°: {max_samples}")
            print(f"ğŸ’¾ ä¿å­˜è®­ç»ƒå†å²: {save_history}")

            for probe_type in probe_types:
                print(f"\n{'='*60}")
                print(f"ğŸ¯ è®­ç»ƒ Probe ç±»å‹: {probe_type}")
                print(f"{'='*60}")

                config.router.probe_type = probe_type

                try:
                    # è®­ç»ƒå¹¶è·å–å†å²è®°å½•
                    history = complete_probe_training_pipeline_with_mixed_datasets(
                        config,
                        task_list=datasets,
                        max_samples=max_samples,
                        mix_strategy="balanced"
                    )

                    print(f"âœ… {probe_type} è®­ç»ƒå®Œæˆ")

                    # ä¿å­˜è®­ç»ƒå†å²
                    if save_history and history:
                        save_training_history(history, probe_type, datasets, max_samples)

                except Exception as e:
                    print(f"âŒ {probe_type} è®­ç»ƒå¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()

    # ==================== æ¨¡å¼: eval_probe ====================
    elif mode == "eval_probe":
        datasets = args.datasets 
        probe_types = args.probe_types
        probe_dir = args.probe_dir
        
        print(f"ğŸ¯ è¯„ä¼° Probe æ¨¡å‹")
        print(f"ğŸ“Š æ•°æ®é›†: {datasets}")
        print(f"ğŸ”§ Probe ç±»å‹: {probe_types}")
        if probe_dir:
            print(f"ğŸ“ Probe ç›®å½•: {probe_dir}")

        if probe_dir:
            probe_files = sorted(glob.glob(f"{probe_dir}/*.pt"))
            print(f"\nåœ¨ {probe_dir} ä¸­æ‰¾åˆ° {len(probe_files)} ä¸ª probe æ–‡ä»¶")

            probe_configs = []
            for pf in probe_files:
                # å°è¯•ä»æ–‡ä»¶åæå– probe_type
                filename = os.path.basename(pf)
                # åŒ¹é…æ¨¡å¼: *_probe_type.pt æˆ– *_train_probe_type.pt
                m = re.search(r'.*?_(?:train_)?([^_]+)\.pt$', filename)
                if m:
                    detected_probe_type = m.group(1)
                    # å¦‚æœæŒ‡å®šäº† probe_typesï¼Œåªå¤„ç†åŒ¹é…çš„ç±»å‹
                    if probe_types and detected_probe_type not in probe_types:
                        continue

                    metric_results_dir = config.metric_results_dir

                    probe_configs.append({
                        "checkpoint_path": pf,
                        "probe_type": detected_probe_type,
                        "metric_results_dir": metric_results_dir,
                    
                    })

            if probe_configs:
                batch_evaluate_probes(config, probe_configs, datasets)
            else:
                print(f"âš ï¸  è­¦å‘Š: åœ¨ {probe_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ probe æ–‡ä»¶")

        # å¦‚æœæ²¡æœ‰æŒ‡å®š probe_dirï¼Œä½¿ç”¨å½“å‰é…ç½®çš„ probe
        else:
            # ä»é…ç½®ä¸­è·å–æ¨¡å‹åç§°
            model_name = _extract_model_name_from_path(config.inference.weak_model_path)

            for probe_type in probe_types:
                print(f"\n{'='*60}")
                print(f" ä½¿ç”¨ Probe ç±»å‹: {probe_type}")
                print(f"{'='*60}")

                config_copy = copy.deepcopy(config)
                config_copy.router.router_type = "probe"
                config_copy.router.probe_type = probe_type
                # å¦‚æœé…ç½®ä¸­å·²æœ‰ checkpoint_pathï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™éœ€è¦ç”¨æˆ·æŒ‡å®š
                if not config_copy.router.checkpoint_path:
                    print(f"âš ï¸  è­¦å‘Š: æœªæŒ‡å®š checkpoint_pathï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®æˆ–ä½¿ç”¨ --probe_dir")
                    continue

                pipeline_test = RouterEvaluationPipeline(config_copy)

                for task in datasets:
                    print(f"\nğŸ“Š è¯„ä¼°ä»»åŠ¡: {task}")

                    hidden_states_file = _build_hs_path(task,model_name)

                    if not os.path.exists(hidden_states_file):
                        print(f"âš ï¸  è­¦å‘Š: Hidden states æ–‡ä»¶ä¸å­˜åœ¨: {hidden_states_file}")
                        continue

                    try:
                        results = pipeline_test.evaluate_complete_pipeline(
                            hidden_states_file=hidden_states_file,
                            datasets=[task]
                        )
                        print(f"âœ… {task} ä½¿ç”¨ {probe_type} è¯„ä¼°å®Œæˆ")
                    except Exception as e:
                        print(f"âŒ {task} ä½¿ç”¨ {probe_type} è¯„ä¼°å¤±è´¥: {e}")

    
    # ==================== æ¨¡å¼: eval_embedding_mlp ====================
    elif mode == "eval_embedding_mlp":
        datasets = args.datasets

        # ä¼˜å…ˆ CLI æä¾›çš„ checkpointï¼Œå¦åˆ™ç”¨é…ç½®
        ckpt_path = args.embedding_checkpoint or config.router.checkpoint_path
        if not ckpt_path:
            print("âŒ eval_embedding_mlp éœ€è¦ checkpointï¼ˆ--embedding_checkpoint æˆ– config.router.checkpoint_pathï¼‰")
            exit(1)
        if not os.path.exists(ckpt_path):
            print(f"âŒ checkpoint æ–‡ä»¶ä¸å­˜åœ¨: {ckpt_path}")
            exit(1)

        # é€‰æ‹© embedding æ–‡ä»¶ï¼šè‹¥ CLI æä¾›åˆ™ç”¨ CLIï¼Œå¦åˆ™æ ¹æ® config.router.embedding_files è‡ªåŠ¨åŒ¹é…
        def _pick_emb_file(task: str):
            if args.query_embeddings_file:
                return args.query_embeddings_file

            task_clean = task.strip()
            exact_stem = f"{task_clean}_query_embeddings"

            # 1) è‹¥é…ç½®äº† embedding_filesï¼Œåˆ™å°è¯•ç²¾ç¡®åŒ¹é…
            files = config.router.embedding_files or []
            if files:
                for fp in files:
                    if Path(fp).stem == exact_stem:
                        return fp

            # 2) åœ¨ç›®å½•é‡Œè‡ªåŠ¨å¯»æ‰¾ï¼ˆé»˜è®¤ç”¨ training.query_embedding_output_dirï¼‰
            base_dir = getattr(config.training, "query_embedding_output_dir", "query_embeddings_output")
            candidate = Path(base_dir) / f"{exact_stem}.pt"
            if candidate.exists():
                return str(candidate)

            # 3) å¦‚æœé…ç½®äº† embedding_files ä½†æ²¡åŒ¹é…åˆ°ï¼Œç»™å‡ºå¯ç”¨åˆ—è¡¨
            available = [Path(fp).stem for fp in files] if files else []
            raise ValueError(
                f"æœªæ‰¾åˆ°ä¸ä»»åŠ¡ {task_clean} åŒ¹é…çš„ embedding æ–‡ä»¶ã€‚"
                f"æœŸæœ›æ–‡ä»¶: {candidate} æˆ– {exact_stem}.ptï¼Œå€™é€‰: {available}"
            )

        config_copy = copy.deepcopy(config)
        config_copy.router.router_type = "embedding_mlp"
        config_copy.router.checkpoint_path = ckpt_path

        pipeline_test = RouterEvaluationPipeline(config_copy)

        for task in datasets:
            emb_file = _pick_emb_file(task)
            if not emb_file:
                print(f"âŒ æœªæ‰¾åˆ° {task} å¯¹åº”çš„ embedding æ–‡ä»¶ï¼ˆè¯·åœ¨ config.router.embedding_files é…ç½®æˆ–ç”¨ --query_embeddings_fileï¼‰")
                continue
            if not os.path.exists(emb_file):
                print(f"âŒ embedding æ–‡ä»¶ä¸å­˜åœ¨: {emb_file}")
                continue

            print(f"\n{'='*60}")
            print(f"ğŸ“Š è¯„ä¼°ä»»åŠ¡: {task}")
            print(f"   ä½¿ç”¨ embedding æ–‡ä»¶: {emb_file}")
            print(f"{'='*60}")

            try:
                pipeline_test.evaluate_complete_pipeline(
                    hidden_states_file=None,
                    datasets=[task],
                    query_embeddings_file=emb_file
                )
                print(f"âœ… {task} è¯„ä¼°å®Œæˆ")
            except Exception as e:
                print(f"âŒ {task} è¯„ä¼°å¤±è´¥: {e}")

    # ==================== æ¨¡å¼: eval_deberta ====================
    elif mode == "eval_deberta":
        datasets = args.datasets
        if config.router.router_type not in ["deberta", "trained_deberta"]:
            print("âš ï¸  å½“å‰é…ç½® router_type ä¸æ˜¯ deberta/trained_debertaï¼Œå°†ç»§ç»­ä½¿ç”¨é…ç½®ä¸­çš„ router_typeã€‚")

        config_copy = copy.deepcopy(config)
        pipeline_test = RouterEvaluationPipeline(config_copy)

        for task in datasets:
            print(f"\n{'='*60}")
            print(f"ğŸ“Š è¯„ä¼°ä»»åŠ¡: {task}")
            print(f"   Router: {config_copy.router.router_type}")
            print(f"{'='*60}")

            try:
                pipeline_test.evaluate_complete_pipeline(
                    hidden_states_file=None,
                    datasets=[task],
                    query_embeddings_file=None
                )
                print(f"âœ… {task} è¯„ä¼°å®Œæˆ")
            except Exception as e:
                print(f"âŒ {task} è¯„ä¼°å¤±è´¥: {e}")

    elif mode == "self_based":
        # ä»é…ç½®ä¸­è·å–æ¨¡å‹åç§°
        model_name = _extract_model_name_from_path(config.inference.weak_model_path)

        strategies = [
            {
                "name": "semantic_entropy",
                "metric_results_dir": "metric_results/base/semantic_entropy",
                "num_samples": 5,
            },
            {
                "name": "self_questioning",
                "metric_results_dir": "metric_results/base/self_questioning",
                "num_samples": 8,
            },
        ]

        eval_datasets = args.datasets

        for strat in strategies:
            print(f"\n{'='*60}")
            print(f"ğŸš€ è¿è¡Œ self-based ç­–ç•¥: {strat['name']}")
            print(f"{'='*60}")

            config_copy = copy.deepcopy(config)
            config_copy.metric_results_dir = strat["metric_results_dir"]
            config_copy.router.router_type = strat["name"]
            config_copy.router.model_path = None
            config_copy.router.num_samples = strat["num_samples"]

            pipeline_test = RouterEvaluationPipeline(config_copy)

            for task in eval_datasets:
                print(f"\nè¯„ä¼°ä»»åŠ¡: {task}")
                hidden_states_file = _build_hs_path(task, model_name)

                if not os.path.exists(hidden_states_file):
                    print(f"è­¦å‘Š: Hidden states æ–‡ä»¶ä¸å­˜åœ¨: {hidden_states_file}")
                    continue

                datasets = [task]
                pipeline_test.evaluate_complete_pipeline(
                    hidden_states_file=hidden_states_file,
                    datasets=datasets
                )
    elif mode == "logits_based_routers":
        from router import RouterManager

        router_manager = RouterManager()
        router_manager.create_max_logits_router()
        router_manager.create_top10_variance_router()
        router_manager.create_coe_router()
        router_manager.create_entropy_router()
        router_manager.create_confidence_margin_router()

        router_types = ["max_logits", "top10_variance", "coe", "entropy", "confidence_margin"]
        eval_datasets = args.datasets 
        for router_type in router_types:
            print(f"\n{'='*60}")
            print(f"ğŸš€ æµ‹è¯•è·¯ç”±å™¨: {router_type}")
            print(f"{'='*60}")

            for task in eval_datasets:
                print(f"\nè¯„ä¼°ä»»åŠ¡: {task}")

                hidden_states_file = _build_hs_path(task)

                if not os.path.exists(hidden_states_file):
                    print(f"è­¦å‘Š: Hidden states æ–‡ä»¶ä¸å­˜åœ¨: {hidden_states_file}")
                    continue

                config_copy = copy.deepcopy(config)
                config_copy.router.router_type = router_type
                config_copy.metric_results_dir = f"metric_results/base/{router_type}"

                pipeline_test = RouterEvaluationPipeline(config_copy)

                datasets = [task]
                pipeline_test.evaluate_complete_pipeline(
                    hidden_states_file=hidden_states_file,
                    datasets=datasets
                )

    else:
        print(f"âŒ æœªçŸ¥æ¨¡å¼: {mode}")
        parser.print_help()
